# 原创
：  【0基础学爬虫】爬虫基础之代理的基本使用

# 【0基础学爬虫】爬虫基础之代理的基本使用

> 
大数据时代，各行各业对数据采集的需求日益增多，网络爬虫的运用也更为广泛，越来越多的人开始学习网络爬虫这项技术，K哥爬虫此前已经推出不少爬虫进阶、逆向相关文章，为实现从易到难全方位覆盖，特设【0基础学爬虫】专栏，帮助小白快速入门爬虫，本期为代理的基本使用。


### 代理概述

ip地址是一个唯一地址，它用于标识互联网或本地网络设备，而代理ip又名代理服务器（Proxy Server），它的主要作用是作为一个中间层，代替用户与目标服务器进行通信。客户端与服务端进行交互时，与抓包工具一样，客户端发起的请求会经过代理服务器，通过代理服务器进行转发，当服务端接收到请求时，获取到的ip就是代理服务器的ip地址，这样就实现了真实ip的隐藏。

### 代理作用

代理ip的应用范围非常广泛，它的主要应用场景有：

**突破访问限制：** 部分网站会根据用户的IP地址进行访问限制，使用代理ip可以绕过这些限制。

**爬虫数据采集：** 在批量数据采集中，爬虫程序需要使用到代理ip来防止被目标网站封禁或限制访问。

**提高网络安全性：** 通过使用代理IP隐藏真实IP地址，可以防止黑客攻击和网络钓鱼等安全威胁。

### 代理分类

代理可以通过不同的特征进行分类，如匿名度、支持协议、地理位置、使用方式、质量等级等。这里主要介绍前三种。

#### 根据匿名度分类

**高匿名代理：** 高匿名代理也叫做完全匿名代理，它完全隐藏了客户端的真实ip地址与其它信息，服务端无法得知请求来自于哪个客户端，只能获取到代理服务器的ip地址。该类型代理速度与稳定性高，但是通过需要付费。

**普通匿名代理：** 普通匿名代理也叫做匿名代理，它会隐藏客户端的ip地址，但是会暴露客户端的其它请求信息，如HTTP请求头信息，服务端可以识别到请求来自代理服务端，但无法追踪到客户端真实ip。该类型代理速度较慢，稳定性较低。

**透明代理：** 透明代理也叫做普通代理，它不会隐藏客户端的ip地址与其它请求信息，服务端可以获取到发起请求的真实ip，因此透明代理没有太大的实际作用，使用较少。

#### 根据支持协议分类

**HTTP代理：** HTTP代理通过http协议来转发请求数据包，主要用于请求web网页，用来访问受限制的网站，提高用户的匿名性。

**HTTPS代理：** HTTPS代理通过https协议来转发请求数据包，它可以帮助客户端与服务端建立安全的通信通道，主要用与加密隐私数据的传输。

**FTP代理：** FTP代理用于转发FTP请求，主要用于数据的上传、下载、缓存。它可以提供访问FTP服务器的匿名性、访问控制、速度优化等功能。

**SOCKS代理：** SOCKS代理可以转发任意类型的网络请求，它支持多种身份验证，是一种通用性的网络代理。

#### 根据地理位置分类

分为国外代理与国内代理

### 代理的使用

上篇文章中，我们讲到了urllib、requests、httpx、aiohttp、websocket这五个网络请求库的使用，这里我们会介绍如何在使用这些网络请求库时设置代理。

**在下文中使用到的代理由 [快代理](https://www.kuaidaili.com/?channelid=zh_kgpc) 提供。**

通常购买代理后平台会提供IP地址、端口号、账号、密码，在代码中设置代理时，代理数据类型通常为字典类型。格式如下：

普通代理格式：

```
proxy = {
    'https://': 'http://%(ip)s:%(port)s' % {"ip":ip,"port":port},
    'http://': 'http://%(ip)s:%(port)s' % {"ip":ip,"port":port}
    }
```

使用账号密码认证方式的格式为：

```
proxy = {
    "http": "http://%(user)s:%(pwd)s@%(ip)s:%(port)s/" % {"user": username, "pwd": password, "ip":ip,"port":port},
    "https": "http://%(user)s:%(pwd)s@%(ip)s:%(port)s/" % {"user": username, "pwd": password, "ip":ip,"port":port}
}
```

如：

```
proxy = {
    'https': 'http://112.74.202.247:16816',
    'http': 'http://112.74.202.247:16816'
}
```

#### urllib

urllib设置代理需要使用到ProxyHandler对象，首先创建一个ProxyHandler对象，然后将代理服务器的地址与端口号传给它。然后使用build_opener方法创建一个Opener对象，将创建好的ProxyHandler传入Opener。最后使用install_opener方法将Opener对象安装为全局Opener，这样在之后的所有urlopen请求中都会使用这个Opener对象。

```
import urllib.request


proxy = {
    'https': 'http://112.74.202.247:16816',
    'http': 'http://112.74.202.247:16816'
}  #代理地址

proxy_handler = urllib.request.ProxyHandler(proxy)
opener = urllib.request.build_opener(proxy_handler)
urllib.request.install_opener(opener)

response = urllib.request.urlopen('http://httpbin.org/get').read().decode('utf-8')

print(response)
# xxx"origin": "112.74.202.247"xxx
```

设置代理请求网页后，可以看到响应信息中origin的值已经变成我们设置的代理ip值了。这样就代表代理ip设置成功了，网页服务端获取到的ip是代理服务器的ip地址。

#### requests

使用urllib设置代理还是比较繁琐的，这也是urllib库的缺点。而在requests中设置代理非常简单，Request对象中提供了一个参数来设置代理。

```
import requests

proxy = {
    'https': 'http://112.74.202.247:16816',
    'http': 'http://112.74.202.247:16816'
}
url = 'http://httpbin.org/get'

response = requests.get(url,proxies=proxy)

print(response.json())
#xxx'origin': '112.74.202.247'xxx
```

#### httpx

httpx功能、用法与requests基本一致，设置方式也是一样的，唯一的区别就在与代理的键名。

```
import httpx

proxy = {
    'https://': 'http://112.74.202.247:16816',
    'http://': 'http://112.74.202.247:16816'
}
url = 'http://httpbin.org/get'

response = httpx.get(url,proxies=proxy)

print(response.json())
#xxx'origin': '112.74.202.247'xxx
```

proxy代理字典中的键名由http、https更改为http:// 和 https:// 。

使用httpx Client设置代理的方式也一样，通过proxies参数进行传递。

```
import httpx

proxy = {
    'https://': 'http://112.74.202.247:16816',
    'http://': 'http://112.74.202.247:16816'
}

with httpx.Client(proxies=proxy) as client:
    response = client.get('https://httpbin.org/get')

print(response.json())
#xxx'origin': '112.74.202.247'xxx
```

#### aiohttp

aiohttp中设置代理与其它库有所区别，代理格式为字符串形式，通过proxy参数进行传递。

```
import aiohttp
import asyncio

async def main():
    async with aiohttp.ClientSession() as session:
        async with session.get('https://httpbin.org/get',proxy=proxy) as response:
            print(await response.json()) ##xxx'origin': '112.74.202.247'xxx

if __name__ == '__main__':

    proxy = "http://112.74.202.247:16816"
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
```

#### websocket

websocket设置代理有两种方式：

第一种

```
from websocket import WebSocketApp

def on_message(ws, message):  #接收到消息时执行
    print(message)

def on_open(ws):  #开启连接时执行
    ws.send("Hello, WebSocket!")  #发送信息

if __name__ == "__main__":
    proxies = {
        "http_proxy_host": "112.74.202.247",
        "http_proxy_port": 16816,
    }

    ws = WebSocketApp("ws://echo.websocket.org/",on_message=on_message)
    ws.on_open = on_open
    ws.run_forever(**proxies)
```

第二种

```
ws = WebSocketApp("ws://echo.websocket.org/",http_proxy_host="112.74.202.247",http_proxy_port=16816)
```

### 总结

在爬虫程序开发中，使用代理是非常重要的。不论是大规模批量采集还是简单的爬虫脚本，使用代理都可以增加程序的稳定性，前提是使用的优质代理，优质代理可以帮助开发者解决很多问题，是爬虫开发者必须学会使用的一种工具。
