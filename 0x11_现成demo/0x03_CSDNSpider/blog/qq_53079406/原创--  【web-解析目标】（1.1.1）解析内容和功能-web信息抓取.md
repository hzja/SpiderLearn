# 原创
：  【web-解析目标】（1.1.1）解析内容和功能：web信息抓取

# 【web-解析目标】（1.1.1）解析内容和功能：web信息抓取

**目录**

[一、web-解析目标](#%E4%B8%80%E3%80%81web-%E8%A7%A3%E6%9E%90%E7%9B%AE%E6%A0%87)

[1.1、第一步：收集信息](#1.1%E3%80%81%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E6%94%B6%E9%9B%86%E4%BF%A1%E6%81%AF)

[1.2、第二步：分析功能](#1.2%E3%80%81%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E5%88%86%E6%9E%90%E5%8A%9F%E8%83%BD)

[1.3、第三步：目标所使用的技术](#1.3%E3%80%81%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E7%9B%AE%E6%A0%87%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E6%8A%80%E6%9C%AF)

[二、Web信息抓取](#%E4%BA%8C%E3%80%81Web%E6%8A%93%E5%8F%96)

[2.1、自动抓取](#2.1%E3%80%81%E8%87%AA%E5%8A%A8%E6%8A%93%E5%8F%96)

[缺点：](#%E7%BC%BA%E7%82%B9%EF%BC%9A)

[2.2、用户指定位置抓取](#2.2%E3%80%81%E7%94%A8%E6%88%B7%E6%8C%87%E5%AE%9A%E4%BD%8D%E7%BD%AE%E7%88%AC%E5%8F%96)

---


## 一、web-解析目标

> 
<h3>1.1、第一步：收集信息</h3>
收集和分析与其有关的一些关键信息
------&gt;了解攻击目标
<hr/>
<h3>1.2、第二步：分析功能</h3>
枚举应用程序的内容与功能（一些功能不明显， 需要猜测和运气）
-------&gt;实际功能与运行机制,功能区域进行分类， 参照各种实例在找不同类型的漏洞
<hr/>
<h3>1.3、第三步：目标所使用的技术</h3>
分析应用程序运行机制的每一个方面、核心安全机制及其（在客户端和服务器上）使用的技术
------&gt;确定攻击面，选择主要目标， 发现可供利用的漏洞


### 1.2、第二步：分析功能

---


---


---


## 二、Web信息抓取

> 
<h3>2.1、自动抓取</h3>
使用各种工具（BurpSuite、WebScarab、ZedAttack Proxy和CAT）自动抓取Web站点的内容。工具首先请求一个Web页面对其进行分析， 查找连接到其他内容的链接，然后再请求这些链接页面，再继续进行这个循环， 直到找不到新的内容为止
<hr/>
分析HTML菜单， 并使用各种预先设定值或随机数将这些表单返回给应用程序， 以扩大搜索范围、浏览多阶段功能、进行基于表单的导航（如什么地方使用下拉列表作为内容菜单）。一些工具还对客户端JavaScript行某种形式的分析， 以提取指向其他内容的URL
<hr/>
许多Web服务器的Web根目录下有一个名为robots. txt的文件， 其中列出了站点不希望Web爬虫访问或坟索引擎列入索引的URL。这个文件中可能包含敏感功能的参考信息等敏感信息
[【robots协议】简介、理解<img alt="" src="https://csdnimg.cn/release/blog_editor_html/release2.1.7/ckeditor/plugins/CsdnLink/icons/icon-default.png?t=M666"/>https://blog.csdn.net/qq_53079406/article/details/125898777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166017886416781667887117%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=166017886416781667887117&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-125898777-null-null.nonecase&amp;utm_term=robot&amp;spm=1018.2226.3001.4450](https://blog.csdn.net/qq_53079406/article/details/125898777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166017886416781667887117%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=166017886416781667887117&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-125898777-null-null.nonecase&amp;utm_term=robot&amp;spm=1018.2226.3001.4450)
<hr/>
使用BurpSuie解析web程序（就是代理后，点开页面，bp会自动爬取，如robots.txt引用的目录未链接到任何地方，可能就爬不出来）


<hr/>
<h4>缺点：</h4>
1、可能无法正确处理不常用的导航机制（使用复杂的JavaScnpt代码动态建立和处理的菜单），会遗漏某个方面的功能
<hr/>
2、可能无法抓取到隐藏在编译客户端对象（如Flash和Java applet）中的链接
<hr/>
3、多阶段功能会执行输入检查（如用户注册表单中可能包含姓名、电子邮件地址， 电话号码和邮政编码字段）， 可能不会接受由自动工具提交的值。
（自动应用程序爬虫会向每一个可编辑的表单字段提交测试字符串，程序返回一条错误消息， 称数据无效，由于爬虫无法处理这种错误消息， 也就无法成功通过注册，无法发现注册后的其他内容或功能）
<hr/>
4、自动化爬虫通常使用URL作为内容标识符。为避免重复的抓取， 如果链接内容已被请求， 它们会识别出来并不会再向其发出诮求。
许多程序使用基于表单的导航机制，相同的URL可能返回不同的内容和功能，就会被遗漏
<hr/>
5、一些应用程序在URL中插入实际上并不用于确定资源或功能的可变数据。每个页面中都可能包含要请求的新URL, 导致不断的抓取
<hr/>
6、应用程序的身份验证机制，要实现有效抓取， 必须能够处理这种机制才能访问它所保护的功能。如果为其手动配置会话令牌或提交给登录功能的证书， 前面提到的爬虫就能实现有效抓取。然而， 即使获得令牌或证书， 由于各种原因， 爬虫执行的一些操作也会让通过验证的会话中断
会访问所有URL, 可能会请求到退出的功能，致使会话中断<br/> 当向某个敏感功能提交无效输入，程序可能会终止会话<br/> 若页面都需使用令牌，无法按正确的顺序请求，导致会话结束


---


---


---


---


> 
<h3>2.2、用户指定位置抓取</h3>
简而言之，就是在bp等软件代理爬取的同时，点开不同的功能页面，进行登录，注册等操作，bp并会继续进行爬取，目的是为了确保收集的数据够丰富，并且不会意外终止发生

