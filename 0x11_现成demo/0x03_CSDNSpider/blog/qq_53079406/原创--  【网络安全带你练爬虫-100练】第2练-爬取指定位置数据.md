# 原创
：  【网络安全带你练爬虫-100练】第2练：爬取指定位置数据

# 【网络安全带你练爬虫-100练】第2练：爬取指定位置数据

**目录**

[一、思路](#%E4%B8%80%E3%80%81%E6%80%9D%E8%B7%AF)

[二、工具](#%E4%BA%8C%E3%80%81%E5%B7%A5%E5%85%B7)

[三、代码处理](#%E4%B8%89%E3%80%81%E4%BB%A3%E7%A0%81%E5%A4%84%E7%90%86)

[第一部分：发起请求+接收响应（不过多讲）](#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E5%8F%91%E8%B5%B7%E8%AF%B7%E6%B1%82%2B%E6%8E%A5%E6%94%B6%E5%93%8D%E5%BA%94%EF%BC%88%E4%B8%8D%E8%BF%87%E5%A4%9A%E8%AE%B2%EF%BC%89)

[第二部分：解析HTML页面+提取数据](#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E8%A7%A3%E6%9E%90HTML%E9%A1%B5%E9%9D%A2%2B%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE)

[第三部分：处理数据](#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE)

---


## 一、思路

分解步骤，化繁为简

爬虫分为五步走：

---


---


## 二、工具

解析数据的工具：如xpath、Beautiful Soup、正则表达式等很多

<u>1、安装Beautiful Soup</u>

是 Python 的一个第三方库，可以用来解析网页数据

```
pip3 install beautifulsoup4
```

官方文档：[https://beautifulsoup.readthedocs.io/zh_CN/latest/](https://beautifulsoup.readthedocs.io/zh_CN/latest/)

<u>2、安装 lxml</u>

**lxml**是python的一个解析库，支持HTML和XML的解析，支持XPath解析方式

```
pip3 install lxml
```

---


---


## 三、代码处理

#### 第一部分：发起请求+接收响应（不过多讲）

```
import requests

if __name__ == '__main__':
    url1="https://beautifulsoup.readthedocs.io/zh_CN/latest/"
    req=requests.get(url=url1)
    req.encoding='utf-8'
    print(req.text)

```

---


#### 第二部分：解析HTML页面+提取数据

我们关心的数据都在标签里面

&lt;div class="section" id="id7"&gt;

div 标签的属性有class、id

属性值分别为：section、id7

```
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    url = "https://beautifulsoup.readthedocs.io/zh_CN/latest/"
    req = requests.get(url)
    req.encoding = 'utf-8'
    html = req.text  # 将获取到的网页内容保存到变量html中
    bs = BeautifulSoup(html, 'lxml')
    text = bs.find('div',id="id7")
    print(text)

```

---


#### 第三部分：处理数据

（1）做到所有最小单位的数据所在的上一级标签

（2）找到每个最小单位的数据所处的同一级标签

 （3）分析最小单位标签内的详细信息所在标签<img alt="" height="932" src="https://img-blog.csdnimg.cn/8c76f897cdfe46f5aff21d0902dea20b.png" width="1200"/>

---


（4）完整代码：

（为了方便大家一步一步来，很多地方没最优化）

```

import re
import requests
from bs4 import BeautifulSoup

def get_TYC_info():
    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36'}
    html = get_page(TYC_url)
    soup = BeautifulSoup(html, 'lxml')
    GS_list = soup.find('div', attrs={'class': 'index_list-wrap___axcs'})
    GS_items = GS_list.find_all('div', attrs={'class': 'index_search-box__7YVh6'})
    for item in GS_items:
        title = item.a.text.replace(item.a.span.text, "")
        link = item.a['href']
        company_type = item.find('div', attrs={'class': 'index_tag-list__wePh_'}).find_all('div', attrs={'class': 'index_tag-common__edIee'})[0].text
        money = item.find('div', attrs={'class': 'index_info-col__UVcZb index_narrow__QeZfV'}).span.text

        print(title.strip())
        print(link)
        print(company_type)
        print(money)

def get_page(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        return response.text
    except:
        return ""


if __name__ == '__main__':
    TYC_url = "https://www.tianyancha.com/search?key=&amp;base=hub&amp;city=wuhan&amp;cacheCode=00420100V2020&amp;sessionNo=1688108233.45545222"
    get_TYC_info()
```

结果如图：<img alt="" height="864" src="https://img-blog.csdnimg.cn/0fa7e29460d1471abe4a55a8c9c0f9b1.png" width="1200"/>

---


（5）逐行解释

1、导入了需要使用的模块：re用于正则表达式操作，requests用于发送HTTP请求，BeautifulSoup用于解析HTML。

```
import re

import requests

from bs4 import BeautifulSoup


```

2、自定义一个函数`get_TYC_info()`，用于获取天眼查（TYC）的信息

```
def get_TYC_info():
```

3、定义了请求头（headers），以模拟浏览器发送请求。

```
headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36'}
```

4、调用自定义的`get_page()`函数，将`TYC_url`作为参数传递给它，并将返回的HTML内容赋值给变量`html`

```
html = get_page(TYC_url)
```

5、使用`BeautifulSoup`模块解析HTML，创建一个BeautifulSoup对象`soup`，参数`'lxml'`表示使用lxml解析器。

```
soup = BeautifulSoup(html, 'lxml')
```

6、从解析后的HTML中找到class属性为`index_list-wrap___axcs`的div元素，并将其赋值给变量`GS_list`

```
GS_list = soup.find('div', attrs={'class': 'index_list-wrap___axcs'})
```

7、从`GS_list`中找到class属性为`index_search-box__7YVh6`的所有div元素，并将它们存储在列表`GS_items`中

```
GS_items = GS_list.find_all('div', attrs={'class': 'index_search-box__7YVh6'})
```

8、遍历`GS_items`列表，对于每个元素，提取标题、链接、公司类型和金额信息，并打印输出。其中，`title`通过替换掉`item.a`下的`span`标签内的文本为空字符串来获得。`link`是`item.a`标签的`href`属性值。`company_type`通过在`item`内进行查找，找到class属性为`index_tag-list__wePh_`的div元素，然后在这个div元素下的所有class属性为`index_tag-common__edIee`的div元素中获取第一个元素的文本内容。`money`通过在`item`内进行查找，找到class属性为`index_info-col__UVcZb index_narrow__QeZfV`的div元素，然后获取其中的`span`标签的文本内容。

```
for item in GS_items: title = item.a.text.replace(item.a.span.text, "") 
    link = item.a['href']
    company_type = item.find('div', attrs={'class': 'index_tag-list__wePh_'}).find_all('div', attrs={'class': 'index_tag-common__edIee'})[0].text 
    money = item.find('div', attrs={'class': 'index_info-col__UVcZb index_narrow__QeZfV'}).span.text print(title.strip()) 

    print(link) 
    print(company_type) 
    print(money)
```

9、自定义了一个名为`get_page()`的函数，用于发送HTTP请求并返回响应的HTML内容。

```
def get_page(url):
```

10、在`get_page()`函数内部，首先定义了请求头（headers），然后使用`requests`模块发送GET请求，传递URL和请求头，并设置超时时间为10秒。如果请求成功，返回响应的HTML内容；如果出现异常，则返回

```
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        return response.text
    except:
        return ""
```

11、入口程序

```
if __name__ == '__main__':
    TYC_url = "https://www.tianyancha.com/search?key=&amp;base=hub&amp;city=wuhan&amp;cacheCode=00420100V2020&amp;sessionNo=1688108233.45545222"
    get_TYC_info()
```

---


---


## 网络安全小圈子

[README.md · 书半生/网络安全知识体系-实战中心 - 码云 - 开源中国 (gitee.com)<img alt="icon-default.png?t=N5K3" src="https://csdnimg.cn/release/blog_editor_html/release2.3.2/ckeditor/plugins/CsdnLink/icons/icon-default.png?t=N5K3"/>https://gitee.com/shubansheng/Treasure_knowledge/blob/master/README.md](https://gitee.com/shubansheng/Treasure_knowledge/blob/master/README.md)

[GitHub - BLACKxZONE/Treasure_knowledge<img alt="icon-default.png?t=N5K3" src="https://csdnimg.cn/release/blog_editor_html/release2.3.2/ckeditor/plugins/CsdnLink/icons/icon-default.png?t=N5K3"/>https://github.com/BLACKxZONE/Treasure_knowledge](https://github.com/BLACKxZONE/Treasure_knowledge)
